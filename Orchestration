Container orchestration is all about managing the life cycle of containers, especally in large and dynamic environment.
* provisoning and deployment of containers
* Scaling up or removieng containers
* movement of containers from one host to another
* Load balacing and service discovery between containers
* Health monitoring of containers and hosts
Minimum of 2 web-server should be running all the time
Example. Amazone ECS task placement. its an orchestration tool
Container orchestration Solutions
 * Docker Swarm, kubernetes, Apaches Mesos, AWS ECS
Managed kubernetes like EKS -: since it would be difficult 

-- docker service ls

docker swarm is a container orchestration tool which is natively supported by docker
swarm cluster -: are the collection of the nodes
Node is an instance of the docker engine participating in the swarm
the manager node dispatches units of work called task to the worker node

--- docker swarm init --advertise-addr 'manager ip'
--- ifconfig
---- ifconfig eth0
---- docker swam join --token 'copy and paste the token generated when creating the manager node'
---- docker node ls
when deleting a  node first it have to be demouted

Service -: the defination of the task to execute on the manager or worker nodes

-- docker service create --name webserver --replicas 2 nginx
-- docker service ps webserver

scaling up in swarm
----docker service scale sevice01=1 service03=3
-----docker service update --replicas 1 service2
the difference is with the first one we can scale multiple services at once.

Global vs replicated service
In global service the task is applied over the entire nodes that are present. for example
when installing antivirus use so global so the antivirus will be applied over the entire node.
a global is a single task so no need for replicas
replicated is simple number of nodes that are running the same task.

--- docker service create --name antivirs --mode global -dt ubuntu

Draining a node.
this is needed when a node is to be updated, restarted. you have to move the task to another 
availible node
  -- docker node update --availability drain swarm03
  -- docker node update --availability active swarm03

Inspect swarm and services
-- docker service ps 'service name'
-- docker service inspect 'service name'
-- docker service inspect 'service name' --pretty
-- docker node inspect 'node name'
-- docker node inspect 'node name' --pretty

 publising ports on the swarm
if the port is not published there would be no way to access the container from the outside so it
need to be publised when creating the service
netstat -ntlp
docker service create --name mywebserver --replicas 2 --publish 8080:80 nginx
ifconfig eth0
curl 172.18.19.160:8080

Docker compose
used when containers have dependencies on other containers
use yaml file to put it togather
--- docker-compose up 'container names'
--- docker-compose down 'container name'
--- docker-compose config  -: check if the syntax is correct
--- docker-compose version
if no container name is given it wil either start all the containes in the docker-compose file
or stop them
docke compose come togather in windows but in lunix you have to download it

Deploying multi service apps in swarm
Containers that have dependencies with other containers so we can use the docker stack
A stack is a group of interrelated services that share dependencies and can be orchestrated and
scaled together
it can compose YAML file like docker compose

-- docker stack deploy --compose-file docker-compose.yml mydemo

Locking swarm cluster
in the cd /var/lab/docker, you can find all the keys there so you have to use the lock feture
--- docker swarm update --help
--- docker swarm update --autolock=true
 SWMKEY-1-9g2WqHql/XH0QmIjCo3bw9VzKV9nG2vRvGHR0fyMtII
--- docker swarm unlock
--- docker swarm unlock-key   to retrive the key
--- docker swarm unlock-ky --rotate
 SWMKEY-1-ZsOw5CbfOMXzAa6ZpvWqqNXfe7t2HUA6vMjZxSrLjIc

Troubelshoot Service Deployments
scenarios can be 1. no node can persform the task of the service. 2. all nodes are drained
3. you reserve specific amount of memory and so on for the service and you dont't have the
resourse specified. 4. some kind of placement constrain
this can lead a service to be on pending state

--- docker inspect 'id of the thing to be inspected'
a json file will be produced and you can see at the status part if there is error and so on.

applying constrains to the service with 
-- docker service create --name troubleshoot --constraint=node.labels.region==1oslo --replicas 3 nginx

Mounting volums in swarm

-- docker service create --name myservice --mount type=volume,source=myvolume,target=/mypath nginx
-- docker container exec -it 'container id' bash
 ls -l / to see mypath that is specified when creating the service
cd /mypath/    enter the mypath folder
touch test.txt   create a file called test.txt
-- docker volume ls
cd /var/lib/docker/volumes/   go in to the volumes that ares created
if the container fail since the volume is used as storage the date wont be lost
even if you remove the service the volume will still be there.

control service placement
there are different ways fou you to control the scale and placement of services on different nodes
1. replicated and global service, 2. Resourse constaint(cpu etc) 3. placement contrainct(only runs
on node with label pci_compliance=true) 4. placement preferences
-- docker node update --label-add region=oslo 'node id'
-- --docker service create --name myconstraint --constraint node.labels.region==oslo --replicas 3 nginx
-- docker node inspect 'node id' -: heare we can find the label to apply on the node.label.region

 Overlya networks
it is the way containers in a dristributed network communicate.
The overlay network driver creates a distrbiuted network among multiple docker daemon hosts
overlay network allows containers connected to it to communicate securly
it connects nodes in a swarm. when ever you create a swarm with nodes the overlay driver is
by default translated

-- docker network ls
-- docker network inspect 'network name or id'
on the out put there is peers which show all the ip and names of the nodes
we can ping from the one node to another by using the ip address. this is because the overly
have made it possible to connect securly

--ping 172.18.19.160 

-- docker network create --driver overlay mynetwork
-- docker service create --name overlayservice --network mynetwork --replicas 3 nginx

Secure Overlay Networks
to enable encryption when you create an overlay network you have to pass the --opt encrypted flag

-- docker network create --opt encrypted --driver overlay my-overlay-secure-network
it creates IPSEC tunnels betweena all nodes it uses AES algorithm in GCM mode and managers
node automatically rotates the key every 12 hrs
overlay encryption is not present in windows

Creating serices using templates
-- docker service create --name nginxservice --hostname="{{.Node.Hostname}}-{{.Service.Name}}" nginx
hostname -: to get the hostname in the terminal
myhostname nginxservice
Service.id,Name,Labels
Node.ID,Hostname
Task.ID,Name,Slot
The above are the things that can be put in the placehoslder
there are three supported flags for the placeholder templates
--hostname, --mount, --env

Split-Brain and Quorum
split brain problem
The master/manager node is connected to the central disk and the worker node is connected to
the central disk through the manager node. if for some reason the connection between the master 
and the worker is at fault, the worker node will assume the manager node is down and it will promot
itself to manager node and directly connect to the centeral disk. now both are manager and both
witll directly access the centeral disk which can lead to data corruption.

the solution is to have a quarom of 3 that is three nodes with one manager.
in quorum all the vm's have a vote of 1. so if there is a failure of connection then the vms
that have connection will have a higer vote. so if the manager node is not connectiong with the
worker node then it only have a vote of one. but the other would have a vote of 2. so the one that
have lost will disconnect from the central disk. then there would be an algorithm and one of the winning
nodes becomes the manager and connect to the central disk.
if the down node starts the connection then it will start as a worker node
in this 3vm's example you can only have failure of one node
we should maintain and odd number of nodes within a cluster
for a quarom of 7 the majority is 4 and for a quarom of 6 is still 4. since we need a majority.

Mnagers nodes are responsible for handling the cluster management tasks.
 - maintaining cluster state
 - scheduling services
 - serving swarm mode HTTP API endpoints
Use a raft implementation, the managers maiantain a consistent internal state of the entire
swarm and all the services running on it
An N number of manager cluster tolerates the loss of at most (N - 1) / 2 managers
if you have 3 managers (3 -1) / 2 = 1 = it can tolerate the failure of one manager node
it will be always rounded to the lower number 1.5 to 1 3.5 to 3

-- docker swarm join-token manager  -: this will give you a token to add a node as a manage node
--  docker swarm join --token SWMTKN-1-40sj8ho0i5o15zwbqto0x4cdbr72cxa5m00t33u5mvcj6dh8ei-03n106ltqpdv47blpt55yoti2 172.18.19.160:2377
 if there are 2 managers then there would be no fault tolerance and if one of the manage fail then
they all fail because (N-1) / 2 becomes 0.5 rounded to 0

 -- docker swarm join-token worker
we can see the token that is genrated for manager and worker are differnt. that is the last
digits/character are the same for all token that are for managers and the same for those that
are workers
docker recomends the maximum number of 7 managers without the performance start degrading




